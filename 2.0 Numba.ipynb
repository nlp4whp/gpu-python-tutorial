{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Numba\n",
    "\n",
    "[Numba](http://numba.pydata.org/) is an open source JIT compiler that translates a subset of Python and NumPy code into fast machine code. \n",
    "\n",
    "Numba supports CUDA GPU programming by directly compiling a restricted subset of Python code into CUDA kernels and device functions following the CUDA execution model. Kernels written in Numba appear to have direct access to NumPy arrays. NumPy arrays are transferred between the CPU and the GPU automatically."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What is a kernel?\n",
    "\n",
    "A kernel is similar to a function, it is a block of code which takes some inputs and is executed by a processor.\n",
    "\n",
    "The difference between a function and a kernel is:\n",
    "- A <u>kernel cannot return anything, it must instead modify memory</u>\n",
    "- A <u>kernel must specify its thread hierarchy (threads and blocks)</u>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What are grids, threads and blocks (and warps)?\n",
    "\n",
    "<u>[Threads and blocks](https://en.wikipedia.org/wiki/Thread_block_(CUDA_programming)) are how you instruct you GPU to process some code in parallel</u>. Our GPU is a parallel processor, so we need to specify how many times we want our kernel to be executed.\n",
    "\n",
    "<u>Threads have the benefit of havign some shared cache memory between them, but there are a limited number of cores on each GPU so we need to break our work down into blocks</u> which will be run after another on the GPU.\n",
    "\n",
    "<figure>\n",
    "\n",
    "![CPU GPU Comparison](images/threads-blocks-warps.png)\n",
    "\n",
    "<figcaption style=\"text-align: center;\"> \n",
    "    \n",
    "Image source <a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/\">https://docs.nvidia.com/cuda/cuda-c-programming-guide/</a>\n",
    "    \n",
    "</figcaption>\n",
    "</figure>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What??\n",
    "\n",
    "Don't worry too much about this now. Just take away the idea that **we need to specify the number of times we want our kernel to be called**, and that is given as two numbers which are multiplied together to give your overall grid size.\n",
    "\n",
    "Rules of thumb for threads per block:\n",
    "- Should be a round multiple of the warp size (32)\n",
    "- A good place to start is 128-512 but benchmarking is required to determine the optimal value.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hello world"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's dig in with some code and hopefully things will become more clear.\n",
    "\n",
    "To start off let's write a simple CPU based Python function which we will call repeatedly within a [list comprehension](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions). From a Python perspective list comprehensions can be a good jumping off point for parallel computing because they feel somewhat parallel already."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = range(10)\n",
    "\n",
    "def foo(i):\n",
    "    return i\n",
    "    \n",
    "[foo(i) for i in data]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we have our `foo` function return its index value and use a for loop to iterate over our data which is generated by `range`.\n",
    "\n",
    "Next we will step by step convert this over to a CUDA kernel and run it on our GPU with numba CUDA."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we need to remember that our <u>kernel cannot return anything</u>. Instead we will use an output list to store the values we would return."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = range(10)\n",
    "output = []\n",
    "\n",
    "def foo(i):\n",
    "    output.append(i)\n",
    "    \n",
    "[foo(i) for i in data]\n",
    "\n",
    "output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our next challenge is that our output array on our GPU must have a fixed length. We can't start off with an empty array and keep appending things. So let's use numpy to create a `ndarray` with the same length as our input data. We will also convert our input list to a `numpy` array too as that's what we can move to our GPU."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = np.asarray(range(10))\n",
    "output = np.zeros(len(data))\n",
    "\n",
    "def foo(i):\n",
    "    output[i] = i\n",
    "    \n",
    "[foo(i) for i in data]\n",
    "\n",
    "output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that our pure Python function behaves like a kernel let's use numba to convert it into one."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from numba import cuda\n",
    "# (warning reason](https://stackoverflow.com/questions/70289909/what-does-it-mean-by-say-gpu-under-ultilization-due-to-low-occupancy)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = np.asarray(range(10))\n",
    "output = np.zeros(len(data))\n",
    "\n",
    "@cuda.jit\n",
    "def foo(input_array, output_array):\n",
    "    i = cuda.grid(1)    # thread id\n",
    "    # output_array[i] = i\n",
    "    output_array[i] = input_array[i] * 2\n",
    "    \n",
    "foo[1, len(data)](data, output)  # foo[blocks, threads](...)\n",
    "\n",
    "print(data, output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Woo the above code ran on our GPU!**\n",
    "\n",
    "Now let's unpack this a bit.\n",
    "\n",
    "To convert our CPU function into a GPU kernel we need to add the `@cuda.jit` decorator. This tells numba to compile our code down to CUDA compatible byte code at runtime.\n",
    "\n",
    "Next we changed our kernels inputs to `input_array` and `output_array`. This is because our kernel needs a reference to both arrays in order to interact with them. (More on this later).\n",
    "\n",
    "**But what about `i`?** Instead of passing our function the index each time we call it we can **rely on a nice CUDA function called `cuda.grid` which allows our kernel to get it's own thread index** while it is running.\n",
    "\n",
    "Lastly we make a funny looking function call `foo[blocks, threads](input, output)`. In order to run our kernel on the GPU in parallel we need to specify how many times we want it to run. Kernel functions are configured using square brackets and passing the block size and thread size. With our array only being `10` elements long we specify a blocksize of `1` and a thread size of `10` which means our kernel will be executed `10` times. Then we pass our arguments as normal."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Something a little bigger\n",
    "\n",
    "Now that we've run our first CUDA kernel with Numba let's try something a little bigger.\n",
    "\n",
    "This time we are going to take a large array and double every number in it. We will do it first in pure Python on the CPU and then in a CUDA kernel on the GPU.\n",
    "\n",
    "Let's start with a large `30m` long array of random numbers. And an output array of equal length."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "random_array = np.random.random((30_000_000))\n",
    "random_array"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "output = np.zeros_like(random_array)\n",
    "output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then in Python let's iterate over this array and double each item into the output array. We can time the cell to see how long this takes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "def foo(i):\n",
    "    output[i] = random_array[i] * 2\n",
    "    \n",
    "[foo(i) for i in range(len(random_array))]\n",
    "\n",
    "output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For me this takes around 10 seconds for the CPU to do this calculation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next let's write a CUDA kernel which does exactly the same thing. The only difference to the previous example is that we set out thread size to a fixes value of `128` and then calculate how many blocks we need to cover the whole array."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import math\n",
    "\n",
    "output = np.zeros_like(random_array)\n",
    "\n",
    "threads = 128\n",
    "blocks = math.ceil(random_array.shape[0] / threads)\n",
    "\n",
    "(blocks, threads)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "@cuda.jit\n",
    "def foo(input_array, output_array):\n",
    "    i = cuda.grid(1)\n",
    "    output_array[i] = input_array[i] * 2\n",
    "    \n",
    "foo[blocks, threads](random_array, output)\n",
    "output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hooray this is now orders of magnitude faster, only taking a few hundred milliseconds."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The savvy among you may be wondering though, numpy is already a C based optimised library and we are comparing our GPU kernel with some pure Python code. What if we did it in numpy?\n",
    "\n",
    "Well you would be right, for this example <u>numpy is still faster than our GPU</u>."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time \n",
    "\n",
    "random_array * 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "But the reason this is the case is <u>because of memory management</u>."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Memory management\n",
    "\n",
    "Earlier we discussed that the CPU and GPU are effectively two separate computers. Each of these computers has it's own memory. \n",
    "\n",
    "All of the data we've worked with so far has been created with numpy on the CPU. So in order for `numba` to work with this data on the GPU it has been quietly copying data back and forth for us.\n",
    "\n",
    "This data movement comes at a penalty.\n",
    "\n",
    "We also have the option of being in control of the data ourselves. We can explicitly move our numpy arrays to the GPU ahead of time with `cuda.to_device`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gpu_random_array = cuda.to_device(random_array)\n",
    "gpu_output = cuda.to_device(np.zeros_like(random_array))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now if we run our kernel again and pass it our GPU memory arrays we see it does in fact out perform numpy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "foo[blocks, threads](gpu_random_array, gpu_output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "However our output result is also still on the GPU. We explicitly copied it there, so we need to explicitly copy it back with `copy_to_host()`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gpu_output.copy_to_host()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Both of these data movement operations take time. But the calculation we are performing here is trivial. As the calculation within our kernel gets more complex the percentage of time spent copying data becomes smaller and smaller.\n",
    "\n",
    "Memory management is useful in other places too. We may wish to write code where we write many kernels and chain them together. It would be inefficient to copy that data to the GPU and back again between each kernel call.\n",
    "\n",
    "```python\n",
    "# move array to GPU\n",
    "foo[blocks, threads](data, output)\n",
    "# move data back to CPU\n",
    "\n",
    "# move array to GPU\n",
    "bar[blocks, threads](data, output)\n",
    "# move data back to CPU\n",
    "\n",
    "# move array to GPU\n",
    "baz[blocks, threads](data, output)\n",
    "# move data back to CPU\n",
    "```\n",
    "\n",
    "So by explicitly putting our data there we can cut down on this time and be more in control of our computation.\n",
    "\n",
    "```python\n",
    "# move array to GPU\n",
    "data = cuda.to_device(data)\n",
    "output = cuda.to_device(output)\n",
    "\n",
    "foo[blocks, threads](data, output)\n",
    "bar[blocks, threads](data, output)\n",
    "baz[blocks, threads](data, output)\n",
    "\n",
    "# move data back to CPU\n",
    "data = data.copy_to_host()\n",
    "output = output.copy_to_host()\n",
    "```"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}